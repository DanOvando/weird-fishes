---
title: A Practical Introduction to Stan
author: Dan Ovando
date: '2018-02-16'
slug: a-practical-introduction-to-stan
categories:
  - programming
tags:
  - stan
type: post
draft: true
---



<div id="whats-stan-and-why-use-it" class="section level1">
<h1>What’s Stan and Why Use It?</h1>
<p>Stan is a programming language designed to make statistical modeling easier and faster, especially for Bayesian estimation problems. Stan can help you estimate complex models with large numbers of parameters, and can generally do it faster than alternative like BUGS/JAGS.</p>
<p>That all sounds good, but what why is that useful for me? Suppose you have a hierarchical ecological modeling problem with data clustered by space, time, and species, such as estimating the effect of ocean temperatures on coral growth. You can use Stan to fit that model (and it will likely be faster than BUGS/JAGS if you’re already used to working in those platforms). Suppose just want to use informative priors to help fit a growth model. You can use Stan for that. Suppose you just prefer Bayesian analysis and want to run a simple multiple regression. Stan can do that.</p>
<p>The purpose of this document is not to illustrate or debate Bayesian analysis, but rather assumes a general knowledge and interest in model fitting, and instead seeks to provide a path to get started using Stan for the practical quantitative researcher. This is not a deep dive into the inner workings of Stan and model fitting - explanations and examples are designed to try and help people get the hang of using Stan. There are many great resources to understand the inner workings of Stan and provide higher-level examples; this is intended as a bridge to help get people there.</p>
<div id="why-stan" class="section level2">
<h2>Why Stan?</h2>
<p>If you do any kind of hierarchical modeling (which as ecologists basically everything we do is; e.g. analyzing coral populations across space and time), if you want to incorporate prior knowledge into a new analysis (e.g. other people’s estimates of growth rates for a similar species), if you just have a gnarly model that something like <code>optim</code> can’t handle, or if you just philosophically prefer Bayesian statistics, Bayesian analysis and Stan may be a great tool for you.</p>
<p>Bayesian analysis on its own has a number of appealing traits</p>
<ul>
<li><p>Forces you to very explicitly write out the error structure of your model and prior beliefs</p></li>
<li><p>Credibility intervals mean that you think confidence intervals mean</p></li>
<li><p>Statistically sound method for building in prior knowledge</p></li>
<li><p>Gets you away from the (in my opinion) very confusing world of “fixed” vs “random” effects, since in a Bayesian world all parameters are random, and the question is just how you structure the priors on those parameters</p></li>
<li><p>Diagnostics are much more intuitive than the wild wilderness of Frequentist tests (IMO)</p></li>
</ul>
<p>But like all things comes with tradeoffs</p>
<ul>
<li><p>Computationally intensive (read: can take a lot longer to run, like hours or days compared to seconds to minutes)</p></li>
<li><p>Requires careful thought on priors</p></li>
<li><p>Unfamiliar reviewers might not like it (though you can and should push back on this one, but still, worth thinking about)</p></li>
</ul>
</div>
<div id="how-does-stan-work" class="section level2">
<h2>How does Stan work?</h2>
<p>Stan is a programming language that allows you to write and fit models. It is also “compiled language”, meaning that you have to write a model, then compile it to run it. This is unlike interpreted languages like R that let you more or less run code as you go. This can be a pain if you’re not used to that sort of thing, but it helps make Stan much faster than saying writing the same model in R. We’ll get into this later.</p>
<p>Stan’s key feature are a series of tools for numerical model fitting, i.e. algorithms that help you can’t be fit by analytical methods, or simpler algorithms like those behind <code>optim</code>. While Stan provides a few different mode fitting algorithms, we’re going to focus on the Markov chain Monte Carlo (MCMC) based methods here.</p>
<p>MCMC’s are an incredibly useful class of algorithms that can be used to fit complex models provide Bayesian inference to statistical problems.</p>
<p>An MCMC works more or less like this</p>
<ol style="list-style-type: decimal">
<li><p>Pick some initial parameters</p></li>
<li><p>See how well those parameters fit the data (Calculate the posterior probability of those parameters given your data, the likelihood, and priors)</p></li>
<li><p>Pick a new set of parameters through some function.</p></li>
<li><p>Accept or reject the new parameters by some function proportional to how much the new parameters improve the model fit.</p></li>
<li><p>Repeat the process many many times</p></li>
</ol>
<p>This simple algorithm can be shown to always converge on an approximation of the posterior probability distribution of the model eventually. The challenge then is mostly in designing functions for selecting and accepting/rejecting new values that will help the algorithm converge in your lifetime.</p>
<p>That’s where Stan comes in. If you write the model, Stan has a number of built in algorithms for helping you use MCMC to fit and diagnose that model quickly and efficiently. It’s beyond the scope of this article or my ability to explain perfectly how it does this, but <span class="citation">@Monahan2016</span> provides a really great summary of the mathematics behind Stan’s algorithms, that I will now butcher.</p>
<p>Stan makes use of two main tools to efficiently solve Bayesian problems: Hamiltonian Monte Carlo (HMC) and the no-U-turn sampler (NUTS). A simple MCMC might choose a new parameter value by drawing from a multivariate normal distribution with means set as the last parameter values, with some tuned or supplied covariance matrix. This means that each new parameter value is likely to be highly correlated with the last parameter value, requiring you to draw a large number of samples and then “thin” the samples to create independent draws from the posterior.</p>
<p>HMC and NUTS works a little differently. The details are very complicated, and I recommend reading more on this if you’re interested (see <span class="citation">@Monnahan2016</span>), but the good news is someone else worked out all the details so that you can use it! But it’s good to have a rough sense of what’s going on.</p>
<p>Think of the posterior probability of your model like a small planet with peaks and valleys. The of HMC/NUTS is a an astronaut that lands on this planet, and the astronauts goal is to find the highest peaks in the landscape. But, there’s a catch. The astronaut is blindfolded, but has a sensor that tells her three things: her elevation, her distance from her starting point, and her “energy”. Her energy is the sum of her potential and kinetic energy, and you can overlay isoclines of energy over this surface. So, she could just systematically walk across the landscape, taking an elevation measurement at each and every point. This would work, but would take forever if the landscape is at all big. So instead, she does something like this. She starts traveling in a random direction, with the only rule being she has to keep track of her energy monitor and more or less stay on the same energy isocline. She can then decide to take tiny little steps, or huge gigantic leaps (remember, it’s Mars so the gravity is much lower). If she takes giant leaps, she covers the ground really quickly, but can also really easily deviate off of her energy isocline. If she takes tiny steps, it’s easy to stay on the energy isocline, but takes forever to travel very far. She picks a step size, and starts traveling, keeping track of her distance from her initial starting point, only stopping when her distance from her initial starting point starts to decrease (a U-turn, i.e. when the path she’s on doubles back on itself), or when she deviates off of her energy isocline, or when she has traveled as far as she’s willing to go along any one isocline. Once she’s stopped, she looks at her altimeter. If she’s higher than she was when she started, she marks that spot as a possible peak. If she’s lower than where she started, she marks that spot as part of a peak with probability proportional to how much lower she is than her starting point. If she decides to mark that spot, she then starts a new path from that point. If she doesn’t mark that spot, she returns to her original position, marks that original position again, and tries a new path.</p>
<p>This leaves really one key decision for the astronaut to make: how big should her stepsize be? She makes this decision by tallying the average rate at which she accepts a new position, and she knows from some smart people that an 80% acceptance rate is reflective of a good balance of of taking big enough steps to be efficient but not so large that you miss important features of the landscape. So, she keeps trying new step sizes until she hits that target acceptance rate.</p>
<p>There’s a lot in here then, but to translate this back into practical terms, this means that for the most part, the only key decision that you, the analyst, needs to make in implementing the HMC/NUTS algorithm is what your target acceptance rate is. Stan then does the work for you of finding the right step size to achieve that target acceptance rate during it’s “warmup” period. Once it’s done with that, it enters the “sampling period”, holding that same step size constant.</p>
<p>We’ll get into the implementation details of this later, but that gives a general idea of what’s happening when you call Stan. This has a few really nice features. It’s more efficient than say an MCMC, and the HMC process means that you don’t have to thin anymore, since the HMC itself should more or less pick values that are largely independent of the last value (though this isn’t always true, and sometimes some thinning is needed). This saves you that annoying step in other Monte-Carlo based approached of having to run a boatload of samples and then “waste” them by thinning them down (though HMC still discards a lot of steps in the process). But, it makes it much easier to set a target number of samples from the posterior. In addition, programs such as BUGs like to try and take advantage of Gibbs sampling to speed things up, which requires conjugate priors, giving you an incentive to select conjugate priors for your model. Unfortunately, nature is rarely so kind; we should pick distributions based off reasonable hypotheses about the state of the world, not computational efficient. Stan (i.e HMC) doesn’t care at all whether you are using conjugate priors, encouraging you to find the right model, not the most mathematically convenient model.</p>
<p>So, this all means that you can focus on writing your model, and leave the complicated tuning and running of HMC to NUTS.</p>
</div>
<div id="a-note-on-divergences" class="section level2">
<h2>A Note on Divergences</h2>
<p>HMC uses a process called the “leapfrog integrator” to draw a sketch of the posterior probability surface, since for all but the most trivial model cases there is no analytical solution. The nice thing is, failures in this integrator are identified by “divergences”, which basically mean that the sampler has deviated from that energy isocline we talked about before. So, the model is going along at say an average energy of 10, and then all of a sudden goes to 20, 40, 100, etc. This tells NUTS that something has gone wrong, and it abandons ship. This can be a sign of two things. Either you just need to decrease the step size (which you do by increasing the target acceptance rate), or it can be a sign that there’s a problem in the design of your model. In the later case, this can sometimes be solved by just reparameterizing your model (which we’ll touch on later), but in other cases can be a sign of a fundamental problem in your model (e.g. a population model that goes negative and as a result produces Inf or NA values in your posterior probability).</p>
<p>Understanding and dealing with divergences is a bit of an art, but it’s important to note that Stan will warn you if divergences pop up, and it’s something you have to look into. Divergences mean that the model didn’t correctly survey the posterior probability and so your results may by suspect.</p>
</div>
</div>
<div id="filling-your-stan-toolbox" class="section level1">
<h1>Filling your Stan Toolbox</h1>
<p>A few general notes on Stan syntax are important to note though.</p>
<p>Stan is based off of C++, but is written differently. It has a bunch of features that make it a bit easier to deal with than C++, but also means that unlike other frameworks like TMB, you can’t just Google the C++ solution to any given problem.</p>
<p>A Stan model in a .stan file is broken into a number of “blocks”, each of which define a particular part of the model. There are several different possible blocks, but to start with we’re going to work with the three that every model has to have: <code>data</code>, <code>parameters</code>, and <code>model</code></p>
<pre class="stan"><code>/* 
You can do long blocks of comments
like this
*/


data{
// load in data
}

parameters{
// define parameters the model is trying to estimate

}
model{

// the posterior probability function

}
</code></pre>
<p>You need to pre-allocate space for any variable that you include in your model, by declaring its type and its size. These declarations need to all happen at the start of each block. Stan uses what is called “Strong typing”, meaning basically that it is very strict in terms of what you can do to different kinds of data, and if you try and treat say an array of real numbers like a vector of real numbers it will blow up.</p>
<p>So, to start with our data section. Suppose we have <em>N</em> observations of <em>Y</em> data, where N is an integer (the number of observations), and Y is a vector of real numbers. We would declare those like</p>
<pre class="stan"><code>
data{

int N; // the number of observations

vector[N] Y; //

}
</code></pre>
<p>This tells Stan that we have an integer object N, and we have a vector Y of length N.</p>
<p>Notice that like C++, every statement needs to be closed with a semi-colon <code>;</code>, and that comments are marked by <code>\\</code>.</p>
<p>You can also declare arrays using a slightly different syntax</p>
<pre class="stan"><code>
data{

int N; // the number of observations

vector[N] Y; //

real X[N];

}
</code></pre>
<p>In this case, Y is a vector of size [N,1] (Stan treats Vectors as a matrix based data type)</p>
<p>and X is a an array of 10 1 dimensional objects. That is a pretty confusing distinction, the main difference being that</p>
<ul>
<li><p>vectors allow vector/matrix algebra. You can’t do this with arrays</p></li>
<li><p>arrays allow for integer storage. So, if for example you want to pass a bunch of indices showing which columns correspond to different subgroups, you need to use <code>int index[n];</code> would produce an array of length n each storing one integer</p></li>
</ul>
<p>To give a few illustrations of this</p>
<pre class="stan"><code>
real x[10];

real y[10];

real z[10];

z = x * y
</code></pre>
<p>Would not work</p>
<pre class="stan"><code>
vector[10] x;

vector[10] y;

vector[10] z;

z = x * y
</code></pre>
<p>Also does not work! Why not???</p>
<p>By default, Stan goes with matrix multiplication, and x and y are both [10,1] matrices, which can’t be matrix multiplied together. You can get Stan to do element-wise operations by using .* (or ./ for division).</p>
<p>So this would work</p>
<pre class="stan"><code>
vector[10] x;

vector[10] y;

vector[10] z;

z = x .* y
</code></pre>
<p>One really nice thing about Stan, as opposed to straight up C++, is that it allows for pretty easy indexing.</p>
<p>This works</p>
<pre class="stan"><code>
vector[10] x;

vector[10] y;

vector[5] z;

z = x[1:5] .* y[1:5];
</code></pre>
<p>As does this</p>
<pre class="stan"><code>
vector[10] x;

vector[10] y;

int i[5];

vector[5] z;

i = {1,2,3,4,7};

z = x[i] .* y[i];
</code></pre>
<p>If your code seems like it should be working and it’s not, 9 times out of 10 it’s a problem with these kinds of things (e.g. trying to multiply an array times a vector). One nice feature of running models through Rstudio is if you open up your <code>.stan</code> script in RStudio, inside the IDE there’s a little “check” button in the right-hand corner of the script that will catch a lot of that stuff. It won’t always tell you where the problem is, but it will let you know that your .stan file won’t run as written.</p>
<p>There are a ton of other details out there in terms of structuring and manipulating data, and you can see chapters 3 and 4 of the official <a href="http://mc-stan.org/users/documentation/">Stan documentation</a> for very detailed descriptions, but these examples should get you started with the kinds of operations most of us do.</p>
<p>One other feature worth calling out for a moment is Stan’s ability to set bounds on just about anything. For example, suppose you have data <em>x</em> that you know can only be positive. You can let Stan know this by <code>&lt;lower = 0&gt;</code></p>
<pre class="stan"><code>
data{

int n; // the number of observations

vector[n] y; //

vector&lt;lower = 0&gt;[n] x;

}
</code></pre>
<p>This lets stan know that X is defined on the range <code>[0, Inf]</code></p>
<p>If x has a positive constraint, say 100, you guessed it <code>&lt;lower = 0, upper = 100&gt;</code></p>
<p>When declared in the <code>data</code> block this isn’t all that useful, beside causing Stan to crash if you accidentally pass data in violation of the constraints (which can be handy as a check that you haven’t messed something up in your R script that prepares the data).</p>
<p>Where the bounds get really useful is in the <code>parameters</code> block. For example, suppose we are estimating a standard deviation <span class="math inline">\(\sigma\)</span>. We know that <span class="math inline">\(\sigma\)</span> has to be positive. So, we could do something like estimate <span class="math inline">\(log(\sigma)\)</span> and then use <span class="math inline">\(exp(log(\sigma))\)</span> in our model.</p>
<p>Or, we can just declare the parameter <span class="math inline">\(\sigma\)</span> to have a lower bound of zero</p>
<pre class="stan"><code>
parameters{

  real&lt;lower = 0&gt; sigma;

}
</code></pre>
<p>By doing this, Stan knows not to look for negative values of <span class="math inline">\(\sigma\)</span>, and will even allow us do set normal priors on sigma</p>
<pre class="stan"><code>
model{
  
  sigma ~ normal(0, 2);
  
}</code></pre>
<p>This is equivalent of saying that our prior on sigma is half normal, with standard deviation 2. The above example may have been a bit confusing since we haven’t gotten to the <code>model</code> block yet.</p>
<p>The <code>model</code> block is where we actually define our model (in terms of likelihoods and priors).</p>
<p>There are a few ways to do this, but for now we’ll stick with generally preferred syntax of</p>
<p><code>some_thing ~ some_distribution(distribution_parameters)</code></p>
<p>So in the above example, we are saying that parameter <span class="math inline">\(\sigma\)</span> come from a normal distribution with mean 0 and standard deviation 2. Stan has support for all kinds of different distributions, and you should see the documentation for descriptions and notes on the meaning of different distribution parameters.</p>
<div id="conditional-statements" class="section level2">
<h2>Conditional statements</h2>
<p>Lastly, we should touch on <code>for</code>, <code>while</code>, and <code>if</code> statements. These all work more or less the same as they do in R, thankfully.</p>
<pre class="stan"><code>
model{

real test[10];

int n;

int N;

N = 10;

for (i in 1:10){

print(test[i])

}

n = 1;

while (n &lt;= N) {
  
  n = n + 1; 
  
  print(n)

}

if (n == N){

print(&quot;hooray&quot;)
}

}
</code></pre>
<p>Notice the use of <code>print</code> there. <code>print</code> is a great way to see what’s going on inside your code once you’ve got it at least syntactically correct (meaning it will compile).</p>
</div>
<div id="a-note-on-priors" class="section level2">
<h2>A Note on Priors</h2>
<p>Setting priors is an art and a science that goes well beyond anything we can discuss here, and there are lots of resources out there to help you on this. You’ll notice though that Stan doesn’t force you to specify priors, so it can be tempting to say “hey, I like Stan, but priors scare me, so I just won’t specify any”. By default though, any parameter that isn’t supplied a specific prior gets assigned a uniform prior on the range <span class="math inline">\([-\infty, \infty]\)</span>, which might seem innocuous, but is not in fact an uninformative prior (in fact nothing may technically be), especially if your model is not actually defined along that range (e.g. if your parameter can only be positive).</p>
<p>As such, while Stan won’t force you to do it, you should always take the time to think carefully about your priors and test the affect of your choices on your model outcomes. At the very least remember that not choosing a prior implicitly still chooses a prior.</p>
<p>So, that’s a small look at the key features of writing Stan code. There’s a ton more out there, but those are the basic tools for most coding. Feel free to play around with the <code>scratch.stan</code> file to test out different behaviors.</p>
<pre class="r"><code>beta &lt;- 0.2

sigma &lt;- 0.5

 x &lt;- -1:200

data &lt;- list(y = x * beta + rnorm(length(x), 0, sigma), n = length(x), x = x,
             z = x, g = x)

plot(x,data$y)

scratch &lt;- stan(file = here::here(&#39;scripts&#39;, &#39;scratch.stan&#39;),
                iter = 2000, 
                warmup = 1000,
                data = data)

plot(scratch)</code></pre>
</div>
</div>
<div id="example-model-stock-recruitment-relationships" class="section level1">
<h1>Example Model: Stock Recruitment Relationships</h1>
<p>Now that you’ve hopefully developed a general idea of what Stan is and why you might like to use it, we’re going to dig into actual using it.</p>
<p>This is just generally good practice: before you start frantically coding you should sit down and write out your model. <span class="citation">@Hobbs</span> provides a great introduction on how to do this if you’re not familiar with the process.</p>
<p>You can certainly do your entire analysis in Stan by itself. However, every language has it’s purpose, and the purpose of Stan is not fast and easy data manipulation. The good news is that Stan has clean interfaces with other programming languages like R and Python, allowing you to do a lot of the complex data manipulation in languages better suited to those tasks. You can then pass your processed data to Stan to do the model fitting, and then analyze your results back in say R.</p>
<p>The most useful way to write a Stan model is in its own file, with the extension <code>.stan</code>. For the purposes of this blog, I’m going to embed the Stan code as chunks in the .Rmd though</p>
<p>We’ll learn the basics of Stan by working trying to fill in this model one step at a time, using the fitting of a Beverton-Holt stock-recruitment relationship as an example.</p>
<div id="write-your-model" class="section level2">
<h2>Write your model</h2>
<p>We are going use data from the RAM Legacy Stock Assessment Database to try and fit a Beverton-Holt stock recruitment relationship using the steepness parameterization from <span class="citation">@Dorn2002</span> (for reasons that will become apparent).</p>
<p>To back up, a stock recruit relationship’s job is to say, for a given amount of observed spawning biomass in a fishery, how many new fish (recruits) enter the population.</p>
<p>We can write a general BH model by</p>
<p><span class="math display">\[ R = \frac{\alpha{SSB}}{1 + \beta{SSB}}\]</span></p>
<p>Where SSB is spawning stock biomass (the biomass of reproductively mature fish in the population), <span class="math inline">\(\alpha\)</span> is the maximum average number of recruits (new fish) possible in the population, and <span class="math inline">\(\beta\)</span> is the amount of SSB needed to produce on average <span class="math inline">\(\alpha{/2}\)</span></p>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-15-1.svg" width="960" /></p>
<p>The problem here is that <span class="math inline">\(\beta\)</span> is obviously very specific to each species, making it difficult to really say much about the resilience of a species based on that parameter. To that end <span class="citation">@Mace1988</span> provided a reparameterization of the BH equation using a term called “steepness” (<em>h</em>), which is the slope of the stock-recruitment when SSB is 20% of max (i.e. unfished) SSB. This allows for species with vastly different stock sizes to be compared in terms of their steepness, with species with higher values of steepness being more resilient to fishing than those with lower steepness</p>
<p><span class="math display">\[ \frac{0.8(\alpha){(h){{SSB}}}}{0.2 (\alpha)(1 - h) + (h - 0.2)SSB}\]</span></p>
<pre class="r"><code>alpha &lt;- 1000


cross_df(list(ssb = 0:1000, h = c(0.21,0.6,0.8))) %&gt;% 
  mutate(recruits = (0.8 * alpha * h * ssb) / (
            0.2 * alpha * (1 - h) +
              (h - 0.2) * ssb
          )) %&gt;% 
  ggplot(aes(ssb, recruits, color = factor(h))) + 
  geom_line() + 
  geom_hline(aes(yintercept = alpha), color = &#39;blue&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-16-1.svg" width="960" /></p>
<pre class="r"><code>cross_df(list(ssb = 0:1000, h = c(0.2,0.6,1))) %&gt;% 
  mutate(recruits = (0.8 * alpha * h * ssb) / (
            0.2 * alpha * (1 - h) +
              (h - 0.2) * ssb
          )) %&gt;% 
  ggplot(aes(ssb, recruits, color = factor(h))) + 
  geom_line() + 
  scale_color_discrete(name = &#39;steepness&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-16-2.svg" width="960" /></p>
<p>Our goal then is going to be to try and estimate steepness and <span class="math inline">\(\alpha\)</span> for some real data</p>
</div>
<div id="examine-spawning-and-recruitment-data" class="section level2">
<h2>Examine spawning and recruitment data</h2>
<pre class="r"><code>sr_data &lt;- read_csv(&quot;rlsadb_v4.25_ssb_recruits.csv&quot;) %&gt;% 
  set_names(tolower)

sr_plots &lt;- sr_data %&gt;%
  select(stockid, stocklong, r, ssb) %&gt;% 
  na.omit() %&gt;% 
  nest(-stockid, -stocklong) %&gt;% 
  mutate(sr_plot = map(data, ~ ggplot(.x,aes(ssb,r)) + 
  geom_point()))

# trelliscopejs::trelliscope(sr_plots, name = &#39;sr_plots&#39;, panel_col = &#39;sr_plot&#39;, self_contained = TRUE)</code></pre>
<p>First, let’s take a look at the spawner-recruit data for one stock in particular</p>
<pre class="r"><code>sal_data &lt;- sr_data %&gt;% 
  filter(stockid == &#39;PSALMAKPSWUD&#39;) %&gt;% 
  select(stocklong, year, ssb, r) %&gt;% 
  na.omit()

sal_data %&gt;% 
  ggplot(aes(ssb, r)) + 
  geom_point()</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-18-1.svg" width="960" /></p>
<p>So, there seems to be a relationship, but it’s messy. Our goal now will be to use Stan to estimate a BH model for this stock.</p>
<p>Let’s start by writing out the model for this model. We have three parameters we need to estimate, steepness <em>h</em>, maximum recruitment <span class="math inline">\(\alpha\)</span>, and some error term <span class="math inline">\(\sigma\)</span>.</p>
<p>We can write this as</p>
<p><span class="math display">\[[\alpha,h,\sigma | r] \propto [ r | \alpha,h,\sigma][\alpha][h][\sigma]  \]</span></p>
<p>Where <em>r</em> are our recruitment data. That’s just a conceptual framework for the model; we can write it more clearly by specifying the model in terms of distributions.</p>
<p><span class="math display">\[[\alpha,h,\sigma | r] \propto normal( log(r) | bh(h,\alpha),\sigma) * unif(h|0.2,1) * normal(\alpha|10*max(r),0.1*max(r)) * cauchy(\sigma|0,5)\]</span></p>
<p>In English, this says that we believe that recruitment is a log-normal process, while specifying some plausible priors for our other parameters. E.g. we know that <em>h</em> has to be between 0.2 and 1, and it’s reasonable to think that max recruitment is something larger than the largest recruitment ever observed (<strong>much more care needs to be taken in prior construction, this is just an example</strong>)</p>
<p>Easiest way to think about the log normal distribution is a normal distribution with a cv instead of a sigma. So, you can just calculate the cv for your data and that should be sigma in log space</p>
</div>
<div id="write-our-stan-model" class="section level2">
<h2>Write our Stan Model</h2>
<p>Now that we know what we need to model, we just have to code it. We’ll work step by step through this process now over in “scripts/bh_model.stan”.</p>
<pre class="stan"><code>
data{

int&lt;lower = 0&gt; n; // number of observations

vector[n] ssb; // vector of observed ssb

vector[n] r; // vector of recruits

real max_r;  // max observed recruitment


}
transformed data{

vector[n] log_r; // log recruitment

log_r = log(r);


}

parameters{

real&lt;lower = 0.2, upper = 1&gt; h; //steepness

real&lt;lower = 0&gt; alpha; // max recruitment

real&lt;lower = 0&gt; sigma; // standard deviation of recruitment


}
transformed parameters{

vector[n] rhat;

vector[n] log_rhat;

rhat = (0.8 * alpha * h * ssb) ./ (0.2 * alpha * (1 - h) +(h - 0.2) * ssb);

log_rhat = log(rhat);

}


model{

log_r ~ normal(log_rhat - 0.5 * sigma^2, sigma);

sigma ~ cauchy(0,2.5);

alpha ~ normal(2*max_r, 0.1*2*max_r);

}

generated quantities{

  vector[n] pp_rhat;

  for (i in 1:n) {

   pp_rhat[i] = exp(normal_rng(log_rhat[i] - 0.5 * sigma^2, sigma));

  }

}
</code></pre>
<p>One confusing thing about the model block. For those of us that come from a population modeling background (or maybe just me), the term “model” implies the structure you use to simulate say a population (e.g. a logistic model). So, my inclination was to put these structures in the <code>model</code> block. This works, but has some undesirable properties, namely that things in the model block don’t get automatically reported. So, if you want to see the vector of predicted population numbers from a model, you’d have to get your parameters from and</p>
<p>Now that we have our <code>.stan</code> file written, we just need to pass out data to it and fit the model. the <code>rstan</code> package makes it really easy to interface between <code>R</code> and <code>Stan</code>.</p>
<p>The first step is passing data from the <code>R</code> environment to <code>Stan</code>. You remember our <code>DATA</code> block in our <code>.stan</code> file? We simply need to pass create a list in <code>R</code> containing named objects matching each of the entries in our <code>DATA</code> block.</p>
<p>Once we’ve done that, we use the <code>stan</code> function to fit our model.</p>
<p>There are a few options that are important to specify in the call to <code>stan</code></p>
<ul>
<li><p>The <code>file</code> entry specifies the path to the <code>.stan</code> file containing your model</p></li>
<li><p><code>data</code> is your list of data</p></li>
<li><p><code>chains</code> specifies the number of chains used in the model fitting. Any actual model run should contain multiple chains to verify convergence, but you can start with one chain for diagnostics. If you have more than one chain, by default <code>stan</code> will run them one after another, so if your model takes a long time this can be daunting. However, you can also specify <code>cores</code>. If you set <code>cores</code> to more than 1, then <code>Stan</code> will run each chain in parallel on different cores. So, if you specify 4 chains and 4 cores, each chain will be run simultaneously on separate cores, so your run time should be about the same as 1 chain on 1 core</p></li>
<li><p><code>warmup</code> is the number of model iterations dedicated to burn-in/tuning/whatever you want to call it. This number defaults to half of <code>iter</code> (the total number of model iterations), but if you start to do large iteration runs (e.g. 20,000), there isn’t necessarily a need to do 10,000 warmup runs. If you’ve tested your model on lower iterations and diagnostics look good with say 1,000 warmup runs, there shouldn’t be any problem with leaving warmup at 1,000 and setting iterations to 20,000; it just gives you a lot more samples to play with.</p></li>
<li><p><code>init</code> allows you to pass initial parameter values for each chain. This is optional, but can help <strong>A LOT</strong>. By default, <code>Stan</code> randomly draws numbers between -2 and 2 for initial values for each parameter. This works if you’re model is reasonably centered. But, if you’re working in a situation where parameters can vary wildly from that (say estimating carrying capacity for a population), this range is going to be a really bad guess if the true parameter value is in the millions. If your model is correctly written, <code>Stan</code> will get to the right result eventually, but it will take a lot longer if you feed it a really poor starting guess. There are a few different ways to set <code>init</code>, I’m just going to cover passing explicit starting guesses. <code>init</code> must by a list of list, of the general form <code>list(chain_1 = list(h = 0.2), chain_2 = list(h = 0.8))</code>. The inner lists contain are names objects for any parameters in the model. In this case, I have a parameter named <code>h</code> in the model, and I’m going to specify an initial guess of <code>h</code> at 0.2 for the first chain, and 0.8 for the second chain. It is very important if you are manually specifying starting guesses to have different initial values for your parameters, since a test of model convergence is whether or not different chains initiated at different values reach the same result. Any parameters you do not manually specify a starting guess for <code>stan</code> goes back to the default random number between -2 and 2</p></li>
</ul>
<pre class="r"><code>warmups &lt;- 1000

total_iterations &lt;- 2000

max_treedepth &lt;-  10

n_chains &lt;-  4

n_cores &lt;- 1

data &lt;- list(n = nrow(sal_data),
               r = sal_data$r,
             ssb = sal_data$ssb,
               max_r = max(sal_data$r)
             )

bh_fit &lt;- rstan::sampling(bhmodel,
                 data = data,
                 chains = n_chains,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = n_cores,
                 refresh = 250,
                 init = list(list(h = 0.4, alpha = 2 * data$max_r),
                             list(h = 0.21, alpha = 3 * data$max_r),
                             list(h = 0.8, alpha = 1 * data$max_r),
                             list(h = 0.3, alpha = .8 * data$max_r)),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;b1a9f6adbebcd3177b418a8a3ea13127&#39; NOW (CHAIN 1).
## 
## Gradient evaluation took 9.3e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.93 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  250 / 2000 [ 12%]  (Warmup)
## Iteration:  500 / 2000 [ 25%]  (Warmup)
## Iteration:  750 / 2000 [ 37%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1250 / 2000 [ 62%]  (Sampling)
## Iteration: 1500 / 2000 [ 75%]  (Sampling)
## Iteration: 1750 / 2000 [ 87%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.150877 seconds (Warm-up)
##                0.138458 seconds (Sampling)
##                0.289335 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b1a9f6adbebcd3177b418a8a3ea13127&#39; NOW (CHAIN 2).
## 
## Gradient evaluation took 2.3e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  250 / 2000 [ 12%]  (Warmup)
## Iteration:  500 / 2000 [ 25%]  (Warmup)
## Iteration:  750 / 2000 [ 37%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1250 / 2000 [ 62%]  (Sampling)
## Iteration: 1500 / 2000 [ 75%]  (Sampling)
## Iteration: 1750 / 2000 [ 87%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.160934 seconds (Warm-up)
##                0.137048 seconds (Sampling)
##                0.297982 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b1a9f6adbebcd3177b418a8a3ea13127&#39; NOW (CHAIN 3).
## 
## Gradient evaluation took 2.4e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  250 / 2000 [ 12%]  (Warmup)
## Iteration:  500 / 2000 [ 25%]  (Warmup)
## Iteration:  750 / 2000 [ 37%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1250 / 2000 [ 62%]  (Sampling)
## Iteration: 1500 / 2000 [ 75%]  (Sampling)
## Iteration: 1750 / 2000 [ 87%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.164426 seconds (Warm-up)
##                0.190746 seconds (Sampling)
##                0.355172 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b1a9f6adbebcd3177b418a8a3ea13127&#39; NOW (CHAIN 4).
## 
## Gradient evaluation took 3.1e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  250 / 2000 [ 12%]  (Warmup)
## Iteration:  500 / 2000 [ 25%]  (Warmup)
## Iteration:  750 / 2000 [ 37%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1250 / 2000 [ 62%]  (Sampling)
## Iteration: 1500 / 2000 [ 75%]  (Sampling)
## Iteration: 1750 / 2000 [ 87%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.164366 seconds (Warm-up)
##                0.155752 seconds (Sampling)
##                0.320118 seconds (Total)</code></pre>
<p>And there we go, we have a model fit! Note, when you’re running <code>stan</code> from a saved <code>.stan</code> file, the format is a little different. Suppose that you have your model saved in <code>scripts/bhmodel.stan</code>. In this case, it’s a bit easier to fir the model use the <code>stan</code> call instead of the <code>sampling</code> call. This would look like</p>
<pre class="r"><code>warmups &lt;- 1000

total_iterations &lt;- 2000

max_treedepth &lt;-  10

n_chains &lt;-  4

n_cores &lt;- 1

data &lt;- list(n = nrow(sal_data),
               r = sal_data$r,
             ssb = sal_data$ssb,
               max_r = max(sal_data$r)
             )

bh_fit &lt;- rstan::stan(file = &quot;scripts/bhmodel.stan&quot;,
                 data = data,
                 chains = n_chains,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = n_cores,
                 refresh = 250,
                 init = list(list(h = 0.4, alpha = 2 * data$max_r),
                             list(h = 0.21, alpha = 3 * data$max_r),
                             list(h = 0.8, alpha = 1 * data$max_r),
                             list(h = 0.3, alpha = .8 * data$max_r)),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))</code></pre>
</div>
<div id="running-diagnostics" class="section level2">
<h2>Running Diagnostics</h2>
<p>Now that we have a model run, it’s time to examine our fits.</p>
<p>In my opinion, <code>rstanarm::launch_shinystan</code> is by far the best way to do this. The folks at <code>Stan</code> built a pretty amazing interface (<code>shinystan</code>) that automatically puts together a wide array of numeric and graphical diagnostics that they recommend running on <code>Stan</code> models.</p>
<p>To get started, I recommend running <code>?launch_shinystan</code> and taking a look at their examples to get a feel for what it can do.</p>
<pre class="r"><code>rstanarm::launch_shinystan(bh_fit)</code></pre>
<p>However, we might also want to be able to run some important diagnostics from within R, either for model comparison or inclusion in reports/publications, so we’ll now look at use the fitted <code>stan</code> model in R.</p>
<p>Calling <code>stan</code>/<code>sampling</code> creates an object of class <code>stanfit</code></p>
<pre class="r"><code>class(bh_fit)</code></pre>
<pre><code>## [1] &quot;stanfit&quot;
## attr(,&quot;package&quot;)
## [1] &quot;rstan&quot;</code></pre>
<p><code>stanfit</code> options are designed to interface with a few base R commands that you’re use to, like <code>summary</code> and <code>plot</code> (though if you have lots of parameters simply calling these can be pretty messy)</p>
<pre class="r"><code>summary(bh_fit,maxsum = 5) %&gt;% 
  str()</code></pre>
<pre><code>## List of 2
##  $ summary  : num [1:109, 1:10] 6.15e-01 1.34e+07 1.24 3.88e+06 2.46e+06 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:109] &quot;h&quot; &quot;alpha&quot; &quot;sigma&quot; &quot;rhat[1]&quot; ...
##   .. ..$ : chr [1:10] &quot;mean&quot; &quot;se_mean&quot; &quot;sd&quot; &quot;2.5%&quot; ...
##  $ c_summary: num [1:109, 1:7, 1:4] 6.21e-01 1.34e+07 1.25 3.95e+06 2.51e+06 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 3
##   .. ..$ parameter: chr [1:109] &quot;h&quot; &quot;alpha&quot; &quot;sigma&quot; &quot;rhat[1]&quot; ...
##   .. ..$ stats    : chr [1:7] &quot;mean&quot; &quot;sd&quot; &quot;2.5%&quot; &quot;25%&quot; ...
##   .. ..$ chains   : chr [1:4] &quot;chain:1&quot; &quot;chain:2&quot; &quot;chain:3&quot; &quot;chain:4&quot;</code></pre>
<p>The plot command is a great way to get a first glance at your fits</p>
<pre class="r"><code>plot(bh_fit) + theme_light()</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-25-1.svg" width="960" /></p>
<p>Before we look at using our model though, let’s take a look at a few diagnostics to try to evaluate our model fit. <strong>DISCLAIMER</strong> You should put a lot more thought and effort in model model diagnosis in real cases, this is just an example of accessing some of the starting points in this process.</p>
<p>The first thing we can check is for the presence of “divergent” transitions (see earlier section for a reminder on what these are). Divergent transition during the sampling period of your model (the iterations after the burn-in) are sign that there maybe a problem with your model. We’ll talk about dealing with these later, but for now here’s how to see if they happened.</p>
<p><code>rstan</code> has a few functions to check these things</p>
<pre class="r"><code>rstan::check_divergences(bh_fit)</code></pre>
<p>We can also look manually at these diagnostics using the output of <code>rstan::get_sampler_params</code>. <code>get_sampler_params</code> returns a list with one object per chain. Each object is a matrix showing diagnostics of each of the stored iterations from the model fitting (by default <code>get_sampler_params</code> includes the warmup iterations, you can set the option <code>inc_warmpup = FALSE</code> to omit these from the report if you want)</p>
<pre class="r"><code>mack_diagnostics &lt;- rstan::get_sampler_params(bh_fit) %&gt;% 
  set_names(1:n_chains) %&gt;% 
   map_df(as_data_frame,.id = &#39;chain&#39;) %&gt;% 
  group_by(chain) %&gt;% 
  mutate(iteration = 1:length(chain)) %&gt;% 
  mutate(warmup = iteration &lt;= warmups)
 

mack_diagnostics %&gt;% 
  group_by(warmup, chain) %&gt;% 
  summarise(percent_divergent = mean(divergent__ &gt;0)) %&gt;% 
  ggplot() +
  geom_col(aes(chain, percent_divergent, fill = warmup), position = &#39;dodge&#39;, color = &#39;black&#39;) + 
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/extract-diagnostics-1.svg" width="960" /></p>
<p>We see then that across all chains, we had no divergences during the sampling period (after the warm-ups), which is what we want to see!</p>
<p><code>treedepth</code> is another really important thing to take a look at. Remember how the HMC algorithm works (more or less). Ideally, a new candidate draw from the parameter space is selected from a place where the likelihood bends back on itself. If you think of the posterior probability space like a circular racetrack, the sampler is a runner on that racetrack. The runner starts off on the left side of the track and starts running north, goes around the bend, and then starts running south. The HMC algorithm would stop and try a new parameter at that point, where the runner has fully turned around. So that sounds great if you’ve got a small track to run on. Suppose though that you are on a 10,000 mile track. Your runner is going to have to run a looooong way before things start to bend around. Or, suppose your runner is on a straight line, the runner is never going to turn around, and so the HMC algorithm would just keep running forever! That’s where the <code>max_treedepth</code> option comes in. HMC will select a candidate parameter value when the parameter space bends back on itself OR when the number of steps specified by <code>max_treedepth</code> is reached. Basically, if the algorithm gets to <code>max_treedepth</code>, the runner says “phew, I’m tired, I’m stopping here”, evaluates that point, and then tries again for another iteration.</p>
<p>By default, <code>max_treedepth</code> is set to 10. So, we should check and make sure that our model isn’t bumping up against <code>max_treedepth</code> a bunch. If it is, that means that the model is selecting candidate draws based on hitting this cap, rather than properties of the posterior probability.</p>
<pre class="r"><code>mack_diagnostics %&gt;% 
  ggplot(aes(iteration, treedepth__, color = chain)) + 
  geom_line() + 
  geom_hline(aes(yintercept = max_treedepth), color = &#39;red&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-27-1.svg" width="960" /></p>
<p>Looks like we’re good, the treedepth of each all of our iterations was below the max_treedepth.</p>
<p>If you’re curious, you can also examine the warmup process through the <code>stepsize</code> parameter. <code>Stan</code> uses the warmup period to tune the <code>stepsize</code> parameter to achieve a target acceptance rate (specified by <code>adapt_delta</code>). You can think of stepsize like resolution. A big <code>stepsize</code> means the model will quickly cover the entire picture of the posterior, but the picture will be really fuzzy, and if the posterior probability surface has important fine scale variation, the model will miss them. A really small stepsize will produce a really high resolution picture, but it will wake a lot longer to make that picture. So, a great feature of <code>stan</code> is it uses this target acceptance rate to find the right stepsize for the model.</p>
<pre class="r"><code>mack_diagnostics %&gt;% 
  ggplot(aes(iteration, stepsize__, color = chain)) + 
  geom_line() </code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-28-1.svg" width="960" /></p>
<p>There are many more diagnostics for the actual sampler, but those are two of the really critical ones. Just because the divergences and treedepth look good doesn’t mean that your model doesn’t have problems that deeper diagnostics would reveal, but seeing problems in those two diagnostics should give you a huge red flag right off the bat.</p>
<p>You can test these things by changing the <code>control</code> options in your call to <code>stan</code>. Try for example setting <code>adapt_delta = 0.5</code> or <code>max_treedepth = 2</code>. You’ll see that you start to develop divergences in the first case, since in order to achieve that target acceptance rate <code>stan</code> sets the stepsize quite large, meaning that you miss important parts of the parameter space and create divergences. In the second case you’ll start to see that the treedepth start so bump up against the <code>max_treedepth</code>.</p>
<p>While these are extreme examples, this also gives an idea of a first step at fixing these problems if they pop up: if you fit a model and you get divergences, the first thing you can try is to increase <code>adapt_delta</code> (in fact, <code>Stan</code> will suggest as much to you if this happens). If you’re bumping up against <code>max_treedepth</code>, increase <code>max_treedepth</code>! If either of these don’t solve the problem, then you’ll need to start think about the specification of your model, which we’ll cover a little later. Note though that both increasing <code>adapt_delta</code> and <code>max_treedepth</code> can dramatically increase your model runtime, so it’s not necessarily a good idea just to crank these things up to high values in order to be conservative.</p>
<div id="parameter-diagnostics" class="section level3">
<h3>Parameter Diagnostics</h3>
<p>Now that we’ve taken a look at the highest-level red flags (divergences and treedepth) and satisfied ourselves that we’re in the clear, we can start to diagnose individual parameter estimates. There are a lot of ways to look at the these, the most useful starting point in my opinion is to extract summary statistics on each model parameter. You can do that by <code>summary(my_model)$summary</code>. <code>summary</code> on it’s own prints out summaries of each parameter, the <code>$summary</code> part allows you to access and store the data behind the printed statistics. As you’ll notice throughout this, I spend a bit of time getting my results out of the <code>stanfit</code> object and into a tidy format. You don’t have to do this, but I find it makes life much easier.</p>
<pre class="r"><code> bh_summary &lt;- summary(bh_fit)$summary %&gt;% 
  as.data.frame() %&gt;% 
  mutate(variable = rownames(.)) %&gt;% 
  select(variable, everything()) %&gt;% 
  as_data_frame()

bh_summary</code></pre>
<pre><code>## # A tibble: 109 x 11
##    variable      mean se_mean       sd   `2.5%`    `25%`    `50%`    `75%`
##    &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1 h          6.15e⁻¹ 2.10e⁻³  8.13e⁻²  4.73e⁻¹  5.57e⁻¹  6.08e⁻¹  6.66e⁻¹
##  2 alpha      1.34e⁺⁷ 2.79e⁺⁴  1.36e⁺⁶  1.07e⁺⁷  1.25e⁺⁷  1.35e⁺⁷  1.44e⁺⁷
##  3 sigma      1.24e⁺⁰ 3.90e⁻³  1.51e⁻¹  9.90e⁻¹  1.14e⁺⁰  1.23e⁺⁰  1.33e⁺⁰
##  4 rhat[1]    3.88e⁺⁶ 2.98e⁺⁴  1.06e⁺⁶  2.41e⁺⁶  3.15e⁺⁶  3.67e⁺⁶  4.39e⁺⁶
##  5 rhat[2]    2.46e⁺⁶ 2.39e⁺⁴  8.15e⁺⁵  1.43e⁺⁶  1.92e⁺⁶  2.28e⁺⁶  2.81e⁺⁶
##  6 rhat[3]    2.92e⁺⁶ 2.63e⁺⁴  9.08e⁺⁵  1.74e⁺⁶  2.31e⁺⁶  2.74e⁺⁶  3.33e⁺⁶
##  7 rhat[4]    1.52e⁺⁶ 1.75e⁺⁴  5.79e⁺⁵  8.42e⁺⁵  1.15e⁺⁶  1.39e⁺⁶  1.74e⁺⁶
##  8 rhat[5]    6.14e⁺⁵ 8.44e⁺³  2.72e⁺⁵  3.24e⁺⁵  4.51e⁺⁵  5.51e⁺⁵  7.01e⁺⁵
##  9 rhat[6]    7.77e⁺⁵ 1.03e⁺⁴  3.34e⁺⁵  4.13e⁺⁵  5.74e⁺⁵  7.00e⁺⁵  8.87e⁺⁵
## 10 rhat[7]    1.78e⁺⁶ 1.95e⁺⁴  6.53e⁺⁵  1.00e⁺⁶  1.36e⁺⁶  1.64e⁺⁶  2.04e⁺⁶
## # ... with 99 more rows, and 3 more variables: `97.5%` &lt;dbl&gt;, n_eff &lt;dbl&gt;,
## #   Rhat &lt;dbl&gt;</code></pre>
<p>There are two really important diagnostic statistics hidden in this summary:</p>
<ul>
<li><p><code>n_eff</code>: the effective sample size</p></li>
<li><p><code>Rhat</code>: the “Gelman and Rubin potential scale reduction statistic”</p></li>
</ul>
<p><code>n_eff</code> measures the effective sample size of that particular parameter. Remember that each iteration of the HMC is based off the parameter value on the previous iteration. Ideally though, through the magic of the NUTS algorithm, if the algorithm works correctly, the parameter chosen at the next iteration will be independent of that early parameter value (this is what “thinning” looks to accomplish in other MCMCs, though you can also thin using HMC). If you’re not doing a very efficient job at sampling the parameter space though, parameter values at a given iteration are more likely to be close to the parameter values at the last iteration. This means that these parameters aren’t really independent, and so if you have 1000 draws from the posterior, you might not actually have 1000 independent samples of the parameter, but rather some smaller number of truly “independent” draws.</p>
<p>So, the <code>n_eff</code> is an estimate of the number of independent samples of a parameter across all chains. In this case, we have 4 chains, with 2000 iterations, half of which are warmup, meaning we sample 1000 iterations in each chain, so the max <code>n_eff</code> possible in this case is 4000</p>
<pre class="r"><code>bh_summary %&gt;% 
  ggplot(aes(n_eff)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = 4000), color = &#39;red&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-30-1.svg" width="960" /></p>
<p>Most of our parameters have a fairly high <code>n_eff</code>, though we see a few are somewhat lower. The <code>Rhat</code> statistic helps tell us whether these parameters are so poorly sampled that we have a problem. More or less <code>Rhat</code> tells you whether or not each of the chains has reached a stable posterior distribution, despite starting at different starting values. Gelman recommends that <code>Rhat</code> for each parameter be less than 1.1</p>
<pre class="r"><code>bh_summary %&gt;% 
  ggplot(aes(Rhat)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = 1.1), color = &#39;red&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-31-1.svg" width="960" /></p>
<p>Looks like we’re good! However, if you’re concerned effective sample size of some of your parameters, the easiest thing to do is simply increase the total number of iterations and especially the warmup period. E.g. in this case we could move to 4000 iterations per chain with 2000 warmup iterations (longer warmup helps tune the algorithm to improve the independence of samples).</p>
<p>So now that we’ve checked some individual parameter diagnostics, we can take a look at our parameter estimates themselves.</p>
<p>Going back to the summary we created, you’ll notice that <code>stan</code> kindly calculated mean values and credible intervals for us.</p>
<p>Remember that we had three parameters in our model, steepness <code>h</code>, max recruits <span class="math inline">\(\alpha\)</span>, and our standard deviation of the log recruits <span class="math inline">\(\sigma\)</span>. Let’s take a look at the fits for those variables</p>
<pre class="r"><code>bh_summary %&gt;% 
  filter(variable %in% c(&#39;h&#39;,&#39;alpha&#39;,&#39;sigma&#39;)) %&gt;% 
  ggplot() + 
  geom_linerange(aes(variable, ymin = `2.5%`,ymax = `97.5%`)) + 
  geom_crossbar(aes(variable, mean, ymin = `25%`, ymax = `75%`), fill= &#39;grey&#39;) + 
  facet_wrap(~variable, scales = &#39;free&#39;)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-32-1.svg" width="960" /></p>
<p>So, we see that most of our credible intervals are fairly tight, though our estimate of unfished recruits <span class="math inline">\(\alpha\)</span> is somewhat wide.</p>
<p>That’s all well and good, but what we might really want to see are the actual recruits estimated by our model. <code>stan</code> is great for this as well: Since we calculated our estimates of recruitment in the <code>transformed parameter</code> block, <code>stan</code> automatically stores the values for recruitment associated with each draw from the posterior, giving us our credible intervals for our recruitment estimates as well!</p>
<p>Our recruitment estimates were stored in the <code>rhat</code> object</p>
<pre class="r"><code>rhat &lt;- bh_summary %&gt;% 
  filter(str_detect(variable,&#39;rhat&#39;) &amp; !str_detect(variable,&#39;log&#39;) &amp; !str_detect(variable,&#39;pp&#39;))

sal_data &lt;- sal_data %&gt;% 
  mutate(mean_rhat = rhat$mean,
         lower = rhat$`2.5%`,
         upper = rhat$`97.5%`)

sal_data %&gt;% 
  ggplot() + 
  geom_point(aes(ssb, r)) + 
  geom_line(aes(ssb, mean_rhat)) + 
  geom_ribbon(aes(ssb, ymin = lower, ymax = upper), alpha = 0.25)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-33-1.svg" width="960" /></p>
<p>Looks good! So, the black line is our model’s estimate of the expected recruits at a given sob, and the grey ribbon corresponds to the 95% credibility interval around this expected value.</p>
<p>If we’re sticklers for the truly raw data, we can also look at the parameter values at each of our samples, rather than using <code>summary</code> to process them for us. We can get at those using <code>rstan::extract()</code>. Note: by default <code>rstan::extract()</code> excludes the warmup iterations, and reshuffles the draws. If you want to keep the draws in their original order (for example to check for autocorrelation), you can set <code>rstan::extract(permuted = FALSE)</code>, and if you want to include the warmup period <code>rstan::extract(inc_warmup = TRUE)</code></p>
<pre class="r"><code>bh_mcmc &lt;- bh_fit %&gt;% 
  rstan::extract()

bh_pars &lt;- bh_mcmc[ c(&#39;h&#39;,&#39;alpha&#39;,&#39;sigma&#39;)] %&gt;% 
  map_df(as_data_frame, .id = &#39;variable&#39;)

bh_pars %&gt;% 
  ggplot(aes(variable,value, fill = variable)) + 
  geom_violin() + 
  facet_wrap(~variable, scales = &#39;free&#39;) </code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-34-1.svg" width="960" /></p>
</div>
<div id="posterior-predictive-analysis" class="section level3">
<h3>Posterior Predictive Analysis</h3>
<p>So far, we’ve fit our model, checked some critical diagnostics, and examined our model fits. <code>Stan</code> also allows us to examine “posterior predictive” fits, an immensely powerful tool in diagnosing Bayesian models, and in using Bayesian models for prediction.</p>
<p>A huge advantage of Bayesian modeling is that it forces us to very explicitly write out our model in terms of our beliefs about the error structures of the model. For example, here we have assumed that our observed recruitment data come from a log-normal distribution, that steepness comes from a uniform distribution on the interval 0.2-1, etc.</p>
<p>Together then, each of these distributions make up the posterior probability distribution, which <code>stan</code> helps us sample from. This also means though that we have a very clear hypothesis about the underlying process generating our data. So, if our hypothesis is right, our model should be able to generate data that looks very similar to the data we actually observed.</p>
<p>In this case, our data are observed recruits. We hypothesize that these observed recruits come from a distribution.</p>
<p><span class="math display">\[log(recruits) \sim normal(bh(h,\alpha,ssb), \sigma)\]</span></p>
<p>So, using our draws from the posterior of h, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\sigma\)</span> and our observed SSB, we can use this model to generate draws of log_recruits, and compare those to the values that we actually see.</p>
<pre class="r"><code>pp_rhat &lt;- bh_mcmc[ &#39;pp_rhat&#39;] %&gt;% 
  map_df(as_data_frame, .id = &#39;variable&#39;) %&gt;% 
  gather(observation,value, -variable)

ggplot() + 
  geom_density(data = pp_rhat, aes(log(value),fill = &#39;Posterior Predictive&#39;), alpha = 0.5) + 
  geom_density(data = sal_data, aes(log(r), fill = &#39;Observed&#39;), alpha = 0.5)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-35-1.svg" width="960" /></p>
<p>So, we see that our data does a reasonable job of reproducing the overall shape of the observed data, a good sign! Though we also see that our model doesn’t do a great job of reproducing that little hump of lower densities observed in the data. We can do a lot more with this type of analysis, for example testing the ability of the posterior predictive to estimate “test statistics” like the min, max, mean, and standard deviation of the observed data, <code>launch_shinystan</code> does several of these for you.</p>
<p>We can also use the posterior predictive to use our model to make predictions. Suppose we monitor another year of data for SSB, and we want to use our model to predict the recruits that that SSB will produce. We can use the posterior predictive to generate replicates from our distribution across our posterior draws of <em>h</em>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\sigma\)</span>. To illustrate this process, we can look at the posterior predictive distributions for the SSBs we used to fit the model</p>
<pre class="r"><code>rhat &lt;- bh_summary %&gt;% 
  filter(str_detect(variable,&#39;rhat&#39;) &amp; !str_detect(variable,&#39;log&#39;) &amp; !str_detect(variable,&#39;pp&#39;))

pp_rhat &lt;- bh_summary %&gt;% 
  filter(str_detect(variable,&#39;pp_rhat&#39;)) %&gt;% 
  mutate(ssb = sal_data$ssb)


sal_data &lt;- sal_data %&gt;% 
  mutate(mean_rhat = rhat$mean,
         lower = rhat$`2.5%`,
         upper = rhat$`97.5%`)

sal_data %&gt;% 
  ggplot() + 
  geom_point(aes(ssb, r)) + 
  geom_line(aes(ssb, mean_rhat)) + 
  geom_ribbon(aes(ssb, ymin = lower, ymax = upper), alpha = 0.25) + 
  geom_line(data = pp_rhat, aes(ssb, mean), color = &#39;red&#39;) +
  geom_ribbon(data = pp_rhat, aes(ssb, ymin = `2.5%`, ymax = `97.5%`), alpha = 0.25, fill = &#39;red&#39;) </code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-36-1.svg" width="960" /></p>
<p>We see that this interval is much broader than our credibility intervals for the mean of the fitted values. This is because the grey shaded area is our credibility interval of the expected values of our model for the observed data. That is different though than our expected value for a new observation of SSB.</p>
<p>All together then, this simple model walks us through the basic steps of using <code>stan</code> to fit models:</p>
<ol style="list-style-type: decimal">
<li><p>Writing our model (in terms of likelihoods)</p></li>
<li><p>Coding our model</p></li>
<li><p>Passing our data from R to Stan</p></li>
<li><p>Performing high-level diagnostics on our model fit (divergence, trees, Rhat, etc.)</p></li>
<li><p>Examining the fitted coefficients of our model</p></li>
<li><p>Examining the posterior predictive statistics</p></li>
<li><p>Using our coefficients for prediction</p></li>
</ol>
<p>Each of these steps warrants more careful consideration than we have gone through here, but this is a solid foundation to base future analysis are, that will work with simple models like this, or much more complex models, the process stays the same.</p>
</div>
</div>
<div id="model-comparison" class="section level2">
<h2>Model Comparison</h2>
<p>So far, we’ve been focused on diagnosing our model to make sure that it has in fact converged and to understand the behavior of our model if it has. However, to put it bluntly, if you’re only writing one model, you’re doing it wrong. We made a huge number of assumptions in the design of this model, from the functional form of our stock-recruitment relationship, to our choices of likelihoods and priors.</p>
<p>Our next step should be to test these assumptions by constructin alternative models and comparing them. Luckily, <code>stan</code> has a number of tools available to help us with model comparison.</p>
<p>The Beverton-Holt model assumes a “compensatory” nature of density dependence. A simple ecological example would be habitat filling: if there is a finite amount of available habitat for recruits, once those spots fill up even if you put more and more eggs into the system the total number of recruits will stay the same. The Ricker model is a bit more flexible, and allows for “depensatory” dynamics, which basically means that the SR curve can start to bend back down. An example of this would be a canabalistic process, where once adult density (SSB) gets high enough, they start to prety on recruits and actually drive recruitment back down.</p>
<p>We can use Stan to test the relative performance of the BH vs Ricker models in explaining the observed patterns of SSB and recruitmenbt.</p>
<p>We’ll use a parameterization of the Ricker curve that still includes steepness</p>
<p><span class="math display">\[ \beta = log(5 * h) / (0.8 * ssb_{max})\]</span></p>
<p><span class="math display">\[\alpha = log(r_{ssb_{max}} /  ssb_{max}) + beta * ssb_{max}\]</span></p>
<p><span class="math display">\[ r = ssb  e^{(alpha - beta * ssb)}\]</span></p>
<pre class="r"><code>rzero &lt;-  1000

szero &lt;- 90

h = 0.9

beta &lt;- log(5 * h) / (0.8 * szero)

ssb &lt;- 0:100

alpha &lt;- log(rzero / szero) + (beta * szero)

recruits &lt;- (ssb * exp(alpha - beta * ssb))

data_frame(ssb = ssb, recruits = recruits) %&gt;% 
  ggplot(aes(ssb, recruits)) +
  geom_point()</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-37-1.svg" width="960" /></p>
</div>
<div id="functions-in-stan" class="section level2">
<h2>Functions in Stan</h2>
<p>Our goal now is to fit a Ricker instead of a BH model to the data. I could of course write another file/model called something like <code>ricker-model.stan</code>. However, you’ll notice that the Ricker and BH model are very similar in many ways, and of course take the same data. So, it would be a lot nicer to write one function that can change what it needs to fit either a BH or Ricker model, but keep the shared components the same. This is a step that I feel is missing in many of the other Stan examples out there: we only see design of the finished model, where what we often need are examples of how to make our code flexible enough to easily consider alternative models. My solution to this is in the following chunk</p>
<pre class="stan"><code>functions{

  vector calc_recruits(vector rec_pars, real h, vector ssb, int n, int bh){

    vector[n] rhat;

    real beta;

    real alpha;

     if (bh == 1) { // if model is in beverton holt mode
      rhat = (0.8 * rec_pars[1] * h * ssb) ./ (0.2 * rec_pars[1] * (1 - h) +(h - 0.2) * ssb);

     } else { // ricker mode

      beta = log(5 * h) / (0.8 * rec_pars[2]);

      alpha = log(rec_pars[1] / rec_pars[2]) + 0.8 * beta * rec_pars[2];

      rhat = ssb .* exp(alpha - beta * ssb);

      }

  return rhat;


} // close function

} // close function block


data{

int&lt;lower = 0&gt; n; // number of observations

int&lt;lower = 0&gt; n_sr_params; // number of observations

vector[n] ssb; // vector of observed ssb

vector[n] r; // vector of recruits

real max_r;  // max observed recruitment

real max_h; // max steepness

int&lt;lower = 0, upper = 1&gt; bh;

real&lt;lower = 0&gt; rec_par_mean[n_sr_params];

real&lt;lower = 0&gt; rec_par_cv[n_sr_params];

} // close data block

transformed data{

vector[n] log_r; // log recruitment

log_r = log(r);

} // close transformed data block

parameters{

real&lt;lower = 0.2, upper = max_h&gt; h; //steepness

real&lt;lower = 0&gt; sigma;

vector&lt;lower = 0 &gt;[n_sr_params] rec_pars;

} // close parameters block

transformed parameters{

vector[n] rhat;

vector[n] log_rhat;

rhat = calc_recruits(rec_pars, h, ssb, n, bh);

log_rhat = log(rhat);

} //close transformed parameters block


model{
log_r ~ normal(log_rhat - 0.5 * sigma^2, sigma);

sigma ~ cauchy(0,2.5);

for (i in 1:n_sr_params){

rec_pars[i] ~ normal(rec_par_mean[i],rec_par_cv[i] * rec_par_mean[i]);

}

} // close model block

generated quantities{

  vector[n] pp_rhat;

  vector[n] log_likelihood;

  for (i in 1:n) {

   pp_rhat[i] = exp(normal_rng(log_rhat[i] - 0.5 * sigma^2, sigma));

  }

   for (i in 1:n) {

   log_likelihood[i] = normal_lpdf(log_r[i] | log_rhat[i] - 0.5 * sigma^2, sigma);

   }

} // close generated quantities block
</code></pre>
<pre class="r"><code>warmups &lt;- 2000

total_iterations &lt;- 4000

max_treedepth &lt;-  10

data &lt;- list(
  n = nrow(sal_data),
  r = sal_data$r,
  ssb = sal_data$ssb,
  max_r = max(sal_data$r),
  bh = 0,
  n_sr_params = 2,
  max_h = 2,
  rec_par_mean = c(2 * max(sal_data$r),  0.5 * max(sal_data$ssb)),
  rec_par_cv = c(0.5, 0.5)
  )

ricker_fit &lt;- rstan::sampling(generic_sr,
                 data = data,
                 chains = 4,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = 1,
                 refresh = 250,
                 init = list(list(h = 0.4, rec_pars = c(2 * data$max_r, 4 * max(data$ssb))),
                             list(h = 0.21, rec_pars = c(1 * data$max_r, 10 *  max(data$ssb))),
                             list(h = 0.8, rec_pars = c(3 * data$max_r, 6 *  max(data$ssb))),
                             list(h = 0.3, rec_pars = c(4 * data$max_r, 5 *  max(data$ssb)))),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 1).
## 
## Gradient evaluation took 4.1e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.334863 seconds (Warm-up)
##                0.306814 seconds (Sampling)
##                0.641677 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 2).
## 
## Gradient evaluation took 1.8e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.374994 seconds (Warm-up)
##                0.643005 seconds (Sampling)
##                1.018 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 3).
## 
## Gradient evaluation took 2.7e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.574742 seconds (Warm-up)
##                0.49405 seconds (Sampling)
##                1.06879 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 4).
## 
## Gradient evaluation took 6.3e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.607993 seconds (Warm-up)
##                0.423092 seconds (Sampling)
##                1.03109 seconds (Total)</code></pre>
<p>You’ll notice below that I’m doing some slightly odd things with arrays in my data call. This is because <code>Stan</code> is much more finicky (or exact if you want to think of it that way) about data types. Part of the appeal of <code>R</code> from a data wrangling perspective is that it is <strong>REALLY</strong> forgiving about data types. R is perfectly happy to construct a vector of length 1, or a 1 by 1 matrix, and these things will more or less behave in the same way (don’t hold me to that). <code>Stan</code> though is a bit pickier. If <code>Stan</code> sees a piece of data that is supposed to be a vector, and sees that it is only length 1, it says “nope, that’s a scalar”, and bad things happen. In this case, I have specified <code>rec_par_mean</code> and <code>rec_par_cv</code> to be real vectors in my <code>DATA</code> block, of length <code>n_sr_params</code>. But, when <code>n_sr_params</code> is 1, and <code>Stan</code> sees that <code>rec_par_mean</code> is just one number, it says OK, this is a scalar, and a scalar can’t have a length, so what’s this dimensions thing doing here? We get arround that by specifying that <code>rec_par_mean</code> is an array with 1 dimension (see <a href="http://mc-stan.org/rstan/reference/stan.html">here</a>, down above the references).</p>
<pre class="r"><code>warmups &lt;- 2000

total_iterations &lt;- 4000

max_treedepth &lt;-  10

data &lt;- list(
  n = nrow(sal_data),
  r = sal_data$r,
  ssb = sal_data$ssb,
  max_r = max(sal_data$r),
  bh = 1,
  n_sr_params = 1,
  max_h = 1,
  rec_par_mean = array(2 * max(sal_data$r), dim = 1),
  rec_par_cv = array(0.5, dim = 1)
  )

bh_fit &lt;- rstan::sampling(generic_sr,
                 data = data,
                 chains = 4,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = 1,
                 refresh = 250,
                 init = list(list(h = 0.4, rec_pars = as.array(2 * data$max_r)),
                             list(h = 0.21, rec_pars = as.array(1 * data$max_r)),
                             list(h = 0.8, rec_pars = as.array(3 * data$max_r)),
                             list(h = 0.3, rec_pars = as.array(4 * data$max_r))),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 1).
## 
## Gradient evaluation took 2e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.228568 seconds (Warm-up)
##                0.403223 seconds (Sampling)
##                0.631791 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 2).
## 
## Gradient evaluation took 2.8e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.421361 seconds (Warm-up)
##                0.498191 seconds (Sampling)
##                0.919552 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 3).
## 
## Gradient evaluation took 2.8e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.43574 seconds (Warm-up)
##                0.435287 seconds (Sampling)
##                0.871027 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;b4edfe9f5d46903d9d3477ef6169bc74&#39; NOW (CHAIN 4).
## 
## Gradient evaluation took 3.1e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  250 / 4000 [  6%]  (Warmup)
## Iteration:  500 / 4000 [ 12%]  (Warmup)
## Iteration:  750 / 4000 [ 18%]  (Warmup)
## Iteration: 1000 / 4000 [ 25%]  (Warmup)
## Iteration: 1250 / 4000 [ 31%]  (Warmup)
## Iteration: 1500 / 4000 [ 37%]  (Warmup)
## Iteration: 1750 / 4000 [ 43%]  (Warmup)
## Iteration: 2000 / 4000 [ 50%]  (Warmup)
## Iteration: 2001 / 4000 [ 50%]  (Sampling)
## Iteration: 2250 / 4000 [ 56%]  (Sampling)
## Iteration: 2500 / 4000 [ 62%]  (Sampling)
## Iteration: 2750 / 4000 [ 68%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3250 / 4000 [ 81%]  (Sampling)
## Iteration: 3500 / 4000 [ 87%]  (Sampling)
## Iteration: 3750 / 4000 [ 93%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.25591 seconds (Warm-up)
##                0.221522 seconds (Sampling)
##                0.477432 seconds (Total)</code></pre>
<pre class="r"><code> bh_summary &lt;- summary(bh_fit)$summary %&gt;% 
  as.data.frame() %&gt;% 
  mutate(variable = rownames(.)) %&gt;% 
  select(variable, everything()) %&gt;% 
  as_data_frame()


 ricker_summary &lt;- summary(ricker_fit)$summary %&gt;% 
  as.data.frame() %&gt;% 
  mutate(variable = rownames(.)) %&gt;% 
  select(variable, everything()) %&gt;% 
  as_data_frame()
 
ricker_rhat &lt;- ricker_summary %&gt;% 
  filter(str_detect(variable,&#39;rhat&#39;) &amp; !str_detect(variable,&#39;log&#39;) &amp; !str_detect(variable,&#39;pp&#39;)) %&gt;% mutate(model = &#39;ricker&#39;,
                                                                                                            ssb = sal_data$ssb)


bh_rhat &lt;- bh_summary %&gt;% 
  filter(str_detect(variable,&#39;rhat&#39;) &amp; !str_detect(variable,&#39;log&#39;) &amp; !str_detect(variable,&#39;pp&#39;)) %&gt;% 
  mutate(model = &#39;bh&#39;,
         ssb = sal_data$ssb)

rhat &lt;- ricker_rhat %&gt;% 
  bind_rows(bh_rhat)

  rhat %&gt;% 
  ggplot() + 
  geom_point(data = sal_data,aes(ssb, r)) + 
  geom_line(aes(ssb, mean, color = model)) + 
  geom_ribbon(aes(ssb, ymin = `2.5%`, ymax = `97.5%`, fill = model),alpha = 0.25)</code></pre>
<p><img src="/blog/2018-02-16-a-practical-introduction-to-stan_files/figure-html/unnamed-chunk-39-1.svg" width="960" /></p>
<p>We now have two alternative models, a Beverton-Holt and a Ricker SR relationship.</p>
<p>As a first step we should of course conduct all the same convergenve tests for the Ricker model that we conducted for the BH model, and we can base some judegement based on those results. For example, if the posterior predictive test look much better for one model that might give us some indication that the data at least support one model over another.</p>
<p>We can also use the <code>loo</code> package to try and quantitatively compare the two models. <code>loo</code> stands for leave-one-out, and the <code>loo</code> function provides a powerful interface for performing leave-one-out cross validation for Bayesian models. Basically, it tests the out-of-sample predictive accuracy of each of the models. You can think of it as an improvement over AIC/DIC for model comparison (see <code>?&quot;loo-package&quot;</code>).</p>
<p>There are a few ways to use <code>loo</code>, but the simplest requires a bit of prep work. <code>loo</code> needs to evaluate the likelihood as a function of leaving out data. So, it needs to have access to the pure likelihood. You can either write a function to do this, which we won’t cover here (see <code>?loo::loo</code>), or you can go back in your model and store the log-likelihood in the <code>generated quantities</code> block</p>
<pre class="stan"><code>
  vector[n] log_likelihood;

  for (i in 1:n) {

   parameter_name[i] = normal_lpdf(log_r[i] | log_rhat[i] - 0.5 * sigma^2, sigma);
    
  }
</code></pre>
<p>Once we’ve done this, we can extract the log-likelihood using <code>loo::extract_log_lik</code>.</p>
<p><code>loo::extract_log_lik()</code> has an option <code>parameter_name</code> that defaults to <code>parameter_name = &quot;log_lik&quot;</code>, but for the sake of this example we’ve named our log-likelood object in the <code>stanfit</code> object <code>parameter_name</code></p>
<pre class="r"><code>log_lik_ricker &lt;- extract_log_lik(ricker_fit, parameter_name = &quot;log_likelihood&quot;)</code></pre>
<p>Once we have this, we can pass the log-likelihood matrix to the <code>loo</code> function to get our diagnostics.</p>
<pre class="r"><code>ricker_loo &lt;- loo::loo(log_lik_ricker)</code></pre>
<p>On their own, these values aren’t too informative for us (in the same way that a lone AIC value doesn’t really tell you much).</p>
<p>But, we can now repeat this process with out BH model, and use <code>loo</code> to compare them.</p>
<pre class="r"><code>log_lik_bh &lt;- extract_log_lik(bh_fit, parameter_name = &quot;log_likelihood&quot;)

bh_loo &lt;- loo::loo(log_lik_bh)</code></pre>
<p>THe output of compare is a big confusing, but basically, if <code>elpd_diff</code> is positive, that means that according to <code>loo</code>, the second model is prefered. If it’s negative, the first model is preferred. So, in this case, per the <code>loo</code> criteria, there is a bit more support for the Beverton-Holt model, at least as we parameterized it here. But we can also see that <code>elpd_diff</code> is on about the same scale as the standard error <code>se</code>, giving some indication that there isn’t a big difference between the models (if for example <code>elpd_diff</code> had been <code>-2000</code>, much bigger than the <code>se</code> of <code>0.5</code>, this would indicate more support for the difference).</p>
<p>If you want to compare more than two models, you just pass more <code>loo</code> objects to compare!</p>
<pre class="r"><code>compare(bh_loo, ricker_loo, ricker_loo)</code></pre>
<pre><code>##            looic se_looic elpd_loo se_elpd_loo p_loo se_p_loo
## bh_loo     115.6   9.7    -57.8      4.9         2.2   0.8   
## ricker_loo 117.2   9.9    -58.6      4.9         2.4   0.9   
## ricker_loo 117.2   9.9    -58.6      4.9         2.4   0.9</code></pre>
<p>Compare conveniently orders the matrix in descending order of model performance.</p>
<p>And just like that, we have a solid sketch of going from raw data, to model fits, to model comparison, using Stan and R. There is clearly a lot more work that would have to go into doing this analysis properly (e.g. we haven’t done any testing of the effects of our choices for our prior distributions), but the tools we’ve gone over here should serve as a useful template to build off of for more complete analysis.</p>
</div>
</div>
